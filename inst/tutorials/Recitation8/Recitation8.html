<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Ozancan Ozdemir" />


<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />
<meta name="learnr-version-prerender" content="0.11.2" />

<title>STAT 412-Recitation 8</title>

<!-- header-includes START -->
<!-- HEAD_CONTENT -->
<!-- header-includes END -->
<!-- HEAD_CONTENT -->

<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<!-- taken from https://github.com/rstudio/rmarkdown/blob/de8a9c38618903627ca509f5401d50a0876079f7/inst/rmd/h/default.html#L293-L343 -->
<!-- tabsets -->
<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>
<!-- end tabsets -->


</head>

<body>
<a class='sr-only sr-only-focusable visually-hidden-focusable' href='#learnr-tutorial-content'>Skip to Tutorial Content</a>



<div class="pageContent band">
<main class="bandContent page">

<article class="topics" id="learnr-tutorial-content">

<div id="section-reminder" class="section level2">
<h2><span style="color:darkred"><strong>Reminder</strong></span></h2>
<p>Last week, we talk about some issues that may need or occur in the
data analysis</p>
<ul>
<li><p>Model Selection with Cross Validation</p></li>
<li><p>Dimension Reduction</p></li>
</ul>
<p>Today, we are talk about categorical data anaylsis and robust
statistics.</p>
<p>Install the packages using the code line given below.</p>
<p><a href="https://www.youtube.com/watch?v=ExLbX8e5kbw">Today’s lecture
song.</a></p>
<pre><code>install.packages(&quot;vcd&quot;,&quot;ggmosaic&quot;,&quot;ltm&quot;,&quot;polycor&quot;,&quot;ISLR&quot;,&quot;caret&quot;,&quot;corrplot&quot;,&quot;MASS&quot;)</code></pre>
<p>Install the <code>InformationValue</code> package by executing the
following command.</p>
<pre><code>devtools::install_version(&quot;InformationValue&quot;, version = &quot;1.2.3&quot;, repos = &quot;http://cran.us.r-project.org&quot;)</code></pre>
<p>install the <code>DMwR</code> package by executing the following
command.</p>
<pre><code>devtools::install_version(&quot;DMwR&quot;, version = &quot;0.4.1&quot;, repos = &quot;http://cran.us.r-project.org&quot;)</code></pre>
</div>
<div id="section-categorical-data-analysis" class="section level2">
<h2><span style="color:darkred"><strong>Categorical Data
Analysis</strong></span></h2>
<p>Categorical variables represent types of data which may be divided
into groups. Examples of categorical variables are race, sex, age group,
and educational level. While the latter two variables may also be
considered in a numerical manner by using exact values for age and
highest grade completed, it is often more informative to categorize such
variables into a relatively small number of groups.</p>
<p>Analysis of categorical data mostly starts with the use of data
tables.</p>
<p><strong>Example</strong></p>
<p>Please load <code>vcd</code> package and call <code>Arthritis</code>
data set. See the variable explanation below.</p>
<ul>
<li><p>ID: patient ID.</p></li>
<li><p>Treatment: indicating treatment (Placebo, Treated).</p></li>
<li><p>Sex: indicating sex (Female, Male).</p></li>
<li><p>Age: age of patient.</p></li>
<li><p>Improved: ordered factor indicating treatment outcome (None,
Some, Marked).</p></li>
</ul>
<pre class="r"><code>library(vcd)
data(Arthritis)
knitr::kable(head(Arthritis))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="left">Treatment</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="left">Improved</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">57</td>
<td align="left">Treated</td>
<td align="left">Male</td>
<td align="right">27</td>
<td align="left">Some</td>
</tr>
<tr class="even">
<td align="right">46</td>
<td align="left">Treated</td>
<td align="left">Male</td>
<td align="right">29</td>
<td align="left">None</td>
</tr>
<tr class="odd">
<td align="right">77</td>
<td align="left">Treated</td>
<td align="left">Male</td>
<td align="right">30</td>
<td align="left">None</td>
</tr>
<tr class="even">
<td align="right">17</td>
<td align="left">Treated</td>
<td align="left">Male</td>
<td align="right">32</td>
<td align="left">Marked</td>
</tr>
<tr class="odd">
<td align="right">36</td>
<td align="left">Treated</td>
<td align="left">Male</td>
<td align="right">46</td>
<td align="left">Marked</td>
</tr>
<tr class="even">
<td align="right">23</td>
<td align="left">Treated</td>
<td align="left">Male</td>
<td align="right">58</td>
<td align="left">Marked</td>
</tr>
</tbody>
</table>
<p>The most basic thing is to create a frequency table. You can do it
using <code>table()</code> function.</p>
<p><strong>Creating a frequency table for single variable</strong></p>
<p>It is also known as one way frequency table.</p>
<pre class="r"><code>table(Arthritis$Sex)</code></pre>
<pre><code>## 
## Female   Male 
##     59     25</code></pre>
<p>You can also create such a table with corresponding proportions.
However, you need to create a one way frequency table as object. After
this, we can use <code>prop.table()</code> function.</p>
<pre class="r"><code>t&lt;-table(Arthritis$Sex)
prop.table(t)</code></pre>
<pre><code>## 
##   Female     Male 
## 0.702381 0.297619</code></pre>
<p><strong>Creating a frequency table for two variables</strong></p>
<p>It is also known as two way frequency table or contingency table. It
is created by <code>table()</code> function. For example, I would like
to create a table for treatment and improved.</p>
<pre class="r"><code>t2&lt;-table(Arthritis$Improved,Arthritis$Treatment)#first input is row, second input is column. 
t2</code></pre>
<pre><code>##         
##          Placebo Treated
##   None        29      13
##   Some         7       7
##   Marked       7      21</code></pre>
<p>As you see from this output, the table does not show the margin sums.
If you are interested in counting margin sums, you can use
<code>margin.table()</code> function. Let’s consider previous</p>
<pre class="r"><code>margin.table(t2,1) #for row sums</code></pre>
<pre><code>## 
##   None   Some Marked 
##     42     14     28</code></pre>
<pre class="r"><code>margin.table(t2,2) #for column sums</code></pre>
<pre><code>## 
## Placebo Treated 
##      43      41</code></pre>
<p><strong>Create a frequency table for 3 variables</strong></p>
<p>It is also known as 3-way table. To do this, we will create a table
object, then use <code>ftable()</code> function.</p>
<p>You can also use <code>structable()</code> function from
<code>vcd</code> package.</p>
<pre class="r"><code>t3&lt;-table(Arthritis$Treatment,Arthritis$Sex,Arthritis$Improved)
#first input is the first row category.
#second input is the second row category.
#third input is the column category.
ftable(t3)</code></pre>
<pre><code>##                 None Some Marked
##                                 
## Placebo Female    19    7      6
##         Male      10    0      1
## Treated Female     6    5     16
##         Male       7    2      5</code></pre>
<p><strong>Visualization of Frequency Table</strong></p>
<p>As said before, the categorical data analysis involves the use of
table. In such cases, visualization of the table plays an important role
in the analysis.</p>
<p>If you would like to visualize the frequency table, you can use
<strong>bar plot</strong></p>
<p>The way of plotting contingency table is <strong>mosaic
plot</strong>. A mosaic plot is a graphical display of the cell
frequencies of a contingency table in which the area of boxes of the
plot are proportional to the cell frequencies of the contingency table.
This procedure can construct mosaic plots for up to four-way contingency
tables.</p>
<p>Consider table for Improved vs Treatment.</p>
<pre class="r"><code>library(ggmosaic)
library(ggplot2)
ggplot(data=Arthritis)+geom_mosaic(aes(x = product(Improved, Treatment)))+labs(x = &quot;Treatment&quot;, y=&quot;Improved&quot;,title=&#39;Improved vs Treatment.&#39;)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-8-1.png" width="624" /></p>
<p>or you can use <code>mosaicplot()</code> base function directly.</p>
<pre class="r"><code>mosaicplot(table(Arthritis$Treatment,Arthritis$Improved),main=&quot;Improved vs Treatment.&quot;)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-9-1.png" width="624" /></p>
<p>You can improve your plot by coloring.</p>
<pre class="r"><code>library(ggmosaic)
ggplot(data=Arthritis)+geom_mosaic(aes(x = product(Improved, Treatment),fill=Improved))+labs(x = &quot;Treatment&quot;, y=&quot;Improved&quot;,title=&#39;Improved vs Treatment.&#39;)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-10-1.png" width="624" /></p>
<pre class="r"><code>mosaicplot(table(Arthritis$Treatment,Arthritis$Improved),main=&quot;Improved vs Treatment.&quot;,col=c(&quot;darkred&quot;,&quot;steelblue&quot;,&quot;orange&quot;))</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-11-1.png" width="624" /></p>
<p>As you see most of the participants having placebo has no
improvement, but most of the treated participant performed a remarkable
improvement.</p>
<p>It is possible to consider more than two categorical variables for
mosaic plot. Now, consider Treatment, Sex and Improved.</p>
<pre class="r"><code>library(ggmosaic)
ggplot(data=Arthritis)+ geom_mosaic(aes(x = product(Improved, Treatment), fill=Improved, conds=product(Sex)))</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-12-1.png" width="624" /></p>
<p>Note that when the number of included variable increases, the plot
becomes more complicated and harder for interpretation.</p>
<p><strong>Measure of Association</strong></p>
<p>Measure of association, in statistics, any of various factors or
coefficients used to quantify a relationship between two or more
variables.</p>
<p><strong>For 2 by 2 Tables</strong></p>
<p><strong>Odds Ratio</strong></p>
<p>An odds ratio (OR) is a statistic that quantifies the strength of the
association between two events, A and B. (Wikipedia, 2020)</p>
<p>Be careful, it is applied for <strong>2x2 contingency
table.</strong></p>
<p>Consider <code>epitools</code> package and use <code>oddsratio</code>
function.</p>
<p>Let’s look at the relationship between treatment and sex.</p>
<pre class="r"><code>or&lt;-table(Arthritis$Sex,Arthritis$Treatment)
or</code></pre>
<pre><code>##         
##          Placebo Treated
##   Female      32      27
##   Male        11      14</code></pre>
<pre class="r"><code>library(epitools)</code></pre>
<pre><code>## Warning: package &#39;epitools&#39; was built under R version 4.1.1</code></pre>
<pre><code>## 
## Attaching package: &#39;epitools&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:vcd&#39;:
## 
##     oddsratio</code></pre>
<pre class="r"><code>oddsratio(or)</code></pre>
<pre><code>## $data
##         
##          Placebo Treated Total
##   Female      32      27    59
##   Male        11      14    25
##   Total       43      41    84
## 
## $measure
##         odds ratio with 95% C.I.
##          estimate     lower    upper
##   Female 1.000000        NA       NA
##   Male   1.497389 0.5804767 3.945381
## 
## $p.value
##         two-sided
##          midp.exact fisher.exact chi.square
##   Female         NA           NA         NA
##   Male    0.4042094    0.4763301  0.3907756
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>The output shows that individuals with placebo are 1.49 times higher
for females compared to males but this is not statistically significant
because the interval for OR includes 1, which indicates that there is no
association.</p>
<p><strong>Relative Risk</strong></p>
<p>Relative risk is a ratio of the probability of an event occurring in
the exposed group versus the probability of the event occurring in the
non-exposed group. It is counted for 2 by 2 tables.</p>
<pre class="r"><code>epitools::riskratio(or,rev=&#39;both&#39;,method = &#39;wald&#39;)</code></pre>
<pre><code>## $data
##         
##          Treated Placebo Total
##   Male        14      11    25
##   Female      27      32    59
##   Total       41      43    84
## 
## $measure
##         risk ratio with 95% C.I.
##          estimate    lower    upper
##   Male   1.000000       NA       NA
##   Female 1.232666 0.747275 2.033341
## 
## $p.value
##         two-sided
##          midp.exact fisher.exact chi.square
##   Male           NA           NA         NA
##   Female  0.4042094    0.4763301  0.3907756
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;Unconditional MLE &amp; normal approximation (Wald) CI&quot;</code></pre>
<p>Males have 1.233 (95% CI 0.747, 2.03) times the risk of treated
compared to females but this is not statistically significant because
the interval for OR includes 1, which indicates that there is no
association.</p>
<p>Note that we can also say the risk of having placebo is 23% higher
for females than males.</p>
<p><strong>Yule’s Q</strong></p>
<p>Yule’s Y, also known as the coefficient of colligation, is a measure
of association between <strong>two binary variables</strong>. It is
similar to correlation between two binary variables.</p>
<pre class="r"><code>library(psych)</code></pre>
<pre><code>## Warning: package &#39;psych&#39; was built under R version 4.1.3</code></pre>
<pre><code>## 
## Attaching package: &#39;psych&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, alpha</code></pre>
<pre class="r"><code>yules_q&lt;-Yule(or)
yules_q</code></pre>
<pre><code>## [1] 0.2026846</code></pre>
<p>As we can say, there is no strong association between participants
sex and treatment type since Yule’s Q is 0.2026846.</p>
<p><strong>For i by j Tables</strong></p>
<p><span class="math inline">\(i\geq 2\)</span>, <span
class="math inline">\(j \geq 2\)</span></p>
<p><strong>Chi-Square Statistics</strong></p>
<p>The Chi Square statistic is commonly used for testing relationships
between categorical variables. The null hypothesis of the Chi-Square
test is that no relationship exists on the categorical variables in the
population; they are independent.</p>
<p>Let’s consider Treatment and Improved. Let’s call the corresponding
table.</p>
<pre class="r"><code>t2</code></pre>
<pre><code>##         
##          Placebo Treated
##   None        29      13
##   Some         7       7
##   Marked       7      21</code></pre>
<p>The hypothesis for this process is as follows;</p>
<p><span class="math inline">\(H_0\)</span> = Treatment and Improvement
are independent.</p>
<p><span class="math inline">\(H_1\)</span> = Treatment and Improvement
are not independent.</p>
<p>You can conduct the test using <code>chisq.test</code> function. The
input of this function is supposed to be <strong>table</strong></p>
<pre class="r"><code>chisq.test(t2)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  t2
## X-squared = 13.055, df = 2, p-value = 0.001463</code></pre>
<p>Since p value less than <span class="math inline">\(\alpha\)</span>,
we reject <span class="math inline">\(H_0\)</span>. Therefore, we are
95% confident that there is a significant relationship between Treatment
and Improvement.</p>
<p><strong>Cramer’s V</strong></p>
<p>In statistics, Cramer’s V ( is a measure of association between two
nominal variables, giving a value between 0 and +1 (inclusive). It is
based on Pearson’s chi-squared statistic. (Wikipedia,2020)</p>
<p>Cramer’s V can be found by using <code>assocstats()</code> in
<code>vcd</code>package. The input of the function is
<strong>table</strong> object.</p>
<pre class="r"><code>library(vcd)
assocstats(t2)</code></pre>
<pre><code>##                     X^2 df  P(&gt; X^2)
## Likelihood Ratio 13.530  2 0.0011536
## Pearson          13.055  2 0.0014626
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.367 
## Cramer&#39;s V        : 0.394</code></pre>
<p>It is seen that the coefficient is 0.394, so we can say that there is
a weak positive relationship between treatment and improvement.</p>
<p><strong>Correlation between a continuous and categorical
variable</strong></p>
<p>If we are interested in the relationship between one continuous and
one <strong>binary</strong> categorical variable (i,e a categorical
variable with two levels), we can use point biserial correlation.</p>
<p>To do so, we use <code>biserial.cor</code> function from ltm
package.</p>
<p>I am interested in the relationship between Age and Treatment. (It is
binary)</p>
<pre class="r"><code>library(ltm)</code></pre>
<pre><code>## Warning: package &#39;ltm&#39; was built under R version 4.1.3</code></pre>
<pre><code>## Zorunlu paket yükleniyor: MASS</code></pre>
<pre><code>## Warning: package &#39;MASS&#39; was built under R version 4.1.3</code></pre>
<pre><code>## Zorunlu paket yükleniyor: msm</code></pre>
<pre><code>## Warning: package &#39;msm&#39; was built under R version 4.1.3</code></pre>
<pre><code>## Zorunlu paket yükleniyor: polycor</code></pre>
<pre><code>## Warning: package &#39;polycor&#39; was built under R version 4.1.3</code></pre>
<pre><code>## 
## Attaching package: &#39;polycor&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     polyserial</code></pre>
<pre><code>## 
## Attaching package: &#39;ltm&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     factor.scores</code></pre>
<pre class="r"><code>bscor&lt;- biserial.cor(Arthritis$Age,Arthritis$Treatment)
bscor</code></pre>
<pre><code>## [1] -0.09448719</code></pre>
<p>As you see, the coefficient is -0.0944872. It indicates that there is
a weak relationship between these two concepts.</p>
<p>If we are interested in the relationship between one continuous and
one <strong>ordinal</strong> variable, we can use biserial
correlation.</p>
<pre class="r"><code>library(polycor)
ps_cor&lt;-polyserial(Arthritis$Age,Arthritis$Improved)
ps_cor</code></pre>
<pre><code>## [1] 0.3148726</code></pre>
<p>As you see, the coefficient is 0.3148726. It indicates that there is
a weak relationship between these two concepts.</p>
<p><strong>Exercise Time</strong></p>
<p>Please click <a
href="http://users.metu.edu.tr/ozancan/exc.zip">here</a> to download the
exercise.</p>
</div>
<div id="section-classification-and-logistic-regression"
class="section level2">
<h2><span style="color:darkred"><strong>Classification and Logistic
Regression</strong></span></h2>
<p><strong>Classification</strong></p>
<p>So far, we have discussed methods that primarily focus on modeling
and predicting quantitative response variables, such as fertility.
Linear regression and related techniques like Ridge and LASSO are
well-suited for these scenarios and perform effectively.</p>
<p>However, when the response variable is <strong>categorical</strong>,
the problem is no longer referred to as a regression problem but is
instead categorized as <strong>a classification problem</strong> In
classification, the objective is to assign each observation to a
specific category or class based on a set of predictor variables,
denoted as X.</p>
<p>The goal of classification is to develop a model that can accurately
classify new observations into predefined categories based on their
predictor variable values. This differs from regression, where the focus
is on predicting a continuous numerical value.</p>
<p>In summary, when dealing with categorical response variables, the
main objective is to perform classification, which involves assigning
observations to specific categories or classes based on predictor
variables.</p>
<p>The simplest classification problem exists when the response variable
has two categories. (Binary response)</p>
<p>In the logit model, the log odds of the outcome is modeled as a
linear combination of the predictor variables.</p>
<p><strong>Logistic Regression</strong></p>
<p>Logistic regression, also called a logit model, is used to model
dichotomous outcome variables. It is developed by David Cox in 1958 for
modeling categorical responses.</p>
<p>For example, statistics students at METU could be categorized as the
students who have Macbook Air (<span class="math inline">\(Y=1\)</span>)
and who do not have Macbook Air (<span
class="math inline">\(Y=0\)</span>)</p>
<p>In the prediction of the such response variables, logistic regression
addresses the problem of estimating a probability <span
class="math inline">\(P(Y=1)\)</span>.</p>
<p><span class="math display">\[
p(x) = P(Y = 1 \mid {X = x})
\]</span></p>
<p>The logistic regression model uses a function, called the logistic
function, to model <span class="math inline">\(P(Y=1)\)</span>.</p>
<p><span class="math display">\[
p(x) = P(Y = 1 \mid {X = x}) = \frac{1}{1+exp-(\beta_0 + \beta_1 x_1 +
\beta_2 x_2 + \cdots  + \beta_p x_p)}
\]</span></p>
<p>With a little bit of algebraic work, the general form of model is
written as follows</p>
<p><span class="math display">\[
\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2
x_2 + \cdots  + \beta_p x_p.
\]</span></p>
<p>Here <span class="math inline">\(p(x)/(1-p(x))\)</span> is called the
odds. Thence, the log odds of the outcome is modeled as a linear
combination of the predictor variables in logistic regression.</p>
<div id="section-application" class="section level3">
<h3><span style="color:darkred"><strong>Application</strong></span></h3>
<p>The motivating example for this lab is the <code>Default</code> data
set from <code>ISLR</code> package.</p>
<p>The data set includes the information on ten thousand customers. The
aim here is to predict which customers will default on their credit card
debt.(i.e cannot pay the dept.)</p>
<pre class="r"><code>library(ISLR)
head(Default)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["default"],"name":[1],"type":["fct"],"align":["left"]},{"label":["student"],"name":[2],"type":["fct"],"align":["left"]},{"label":["balance"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["income"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"No","2":"No","3":"729.5265","4":"44361.625","_rn_":"1"},{"1":"No","2":"Yes","3":"817.1804","4":"12106.135","_rn_":"2"},{"1":"No","2":"No","3":"1073.5492","4":"31767.139","_rn_":"3"},{"1":"No","2":"No","3":"529.2506","4":"35704.494","_rn_":"4"},{"1":"No","2":"No","3":"785.6559","4":"38463.496","_rn_":"5"},{"1":"No","2":"Yes","3":"919.5885","4":"7491.559","_rn_":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Check the dimension of the data.</p>
<pre class="r"><code>dim(Default)</code></pre>
<pre><code>## [1] 10000     4</code></pre>
<p>After this, we should divide our data set as train and test to check
the validity of the model.</p>
<pre class="r"><code>dplyr::glimpse(Default)</code></pre>
<pre><code>## Rows: 10,000
## Columns: 4
## $ default &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No~
## $ student &lt;fct&gt; No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N~
## $ balance &lt;dbl&gt; 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8~
## $ income  &lt;dbl&gt; 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55~</code></pre>
<p>Obtain the summary of the data. (Thereby, we also check the missing
value.)</p>
<pre class="r"><code>summary(Default)</code></pre>
<pre><code>##  default    student       balance           income     
##  No :9667   No :7056   Min.   :   0.0   Min.   :  772  
##  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  
##                        Median : 823.6   Median :34553  
##                        Mean   : 835.4   Mean   :33517  
##                        3rd Qu.:1166.3   3rd Qu.:43808  
##                        Max.   :2654.3   Max.   :73554</code></pre>
<p>The response variable is <code>default</code>. Thus, it is better to
check the proportion of the levels.</p>
<pre class="r"><code>prop.table(table(Default$default))</code></pre>
<pre><code>## 
##     No    Yes 
## 0.9667 0.0333</code></pre>
<pre class="r"><code>set.seed(42)
default_idx = sample(nrow(Default), 8000) #produce index number for train data
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]</code></pre>
<p>Construct the proportion of response variable.</p>
<pre class="r"><code>prop.table(table(default_trn$default))</code></pre>
<pre><code>## 
##    No   Yes 
## 0.966 0.034</code></pre>
<pre class="r"><code>prop.table(table(default_tst$default))</code></pre>
<pre><code>## 
##     No    Yes 
## 0.9695 0.0305</code></pre>
<p>As seen, the proportion of the levels of the response are close to
each other and we kept the original proportion. This has to be in the
regression model.</p>
<p><strong>Simple Logistic Regression</strong></p>
<p>We start with a single predictor example, using <code>balance</code>
as our single predictor.</p>
<p>Fitting this model looks very similar to fitting a simple linear
regression. Instead of <code>lm()</code> we use <code>glm()</code>. The
only other difference is the use of <code>family = "binomial"</code>
which indicates that we have a two-class categorical response. Using
<code>glm()</code> with <code>family = "gaussian"</code> would perform
the usual linear regression.</p>
<pre class="r"><code>model_glm = glm(default ~ balance, data = default_trn, family = &quot;binomial&quot;)</code></pre>
<p>After fitting the model, see its summary.</p>
<pre class="r"><code>summary(model_glm)</code></pre>
<pre><code>## 
## Call:
## glm(formula = default ~ balance, family = &quot;binomial&quot;, data = default_trn)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3063  -0.1438  -0.0568  -0.0212   3.7822  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.081e+01  4.099e-01  -26.36   &lt;2e-16 ***
## balance      5.601e-03  2.493e-04   22.47   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2374.1  on 7999  degrees of freedom
## Residual deviance: 1274.4  on 7998  degrees of freedom
## AIC: 1278.4
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>The probabilty of defaulting on the credit card dept is estimated by
the following equation</p>
<p><span class="math display">\[
\hat P(Default=Yes) = \frac{exp(-10.81
+  0.005601*balance)}{(1+exp(-10.81 +  0.005601*balance))}
\]</span></p>
<p>We start to check the significance of the model. So, the following
hypothesis is conducted and null and residual deviance values are used.
Then, this difference is compared to <span
class="math inline">\(\chi^2_2\)</span>.</p>
<p><span class="math inline">\(H_0\)</span>= <span
class="math inline">\(\beta_0\)</span>=<span
class="math inline">\(\beta_1\)</span>=0 vs <span
class="math inline">\(H_1\)</span>= At least one of them is
different.</p>
<pre class="r"><code>ifelse ((2374.1-1274.4)&lt; qchisq(0.95,2),&quot;Fail to Reject Ho&quot;,&quot;Reject Ho&quot;)</code></pre>
<pre><code>## [1] &quot;Reject Ho&quot;</code></pre>
<p>Instead, you can use the following function.</p>
<pre class="r"><code>LikelihoodRatioTest &lt;- function(model, conf.level = 0.95) {
  dev &lt;- model$null.deviance - model$deviance
  dof &lt;- model$df.null - model$df.residual
  if (dev &gt; qchisq(conf.level, dof)) { print(&quot;At least one of the explanatory variables are significant&quot;)
  } else print(&quot;This model is not significant&quot;)
}</code></pre>
<pre class="r"><code>LikelihoodRatioTest(model_glm)</code></pre>
<pre><code>## [1] &quot;At least one of the explanatory variables are significant&quot;</code></pre>
<p>More statistically, you can also type</p>
<pre class="r"><code>anova(model_glm, test = &quot;Chisq&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Df"],"name":[1],"type":["int"],"align":["right"]},{"label":["Deviance"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Resid. Df"],"name":[3],"type":["int"],"align":["right"]},{"label":["Resid. Dev"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Pr(>Chi)"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"NA","2":"NA","3":"7999","4":"2374.124","5":"NA","_rn_":"NULL"},{"1":"1","2":"1099.696","3":"7998","4":"1274.428","5":"3.845511e-241","_rn_":"balance"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>It is seen that the overall model is significant. After that, the
model coefficient is examined.(See model output.)</p>
<p>There are several ways to interpret the coefficients in the logistic
regression.</p>
<p>Balance has significantly positive effect on the log odds of default
status.</p>
<p>The increment in Balance increase the chance of defaulting on credit
card debt.</p>
<p>More mathematically,</p>
<p>The expected change in log odds of defaulting on credit card debt for
one unit increase in the average balance is 0.0056.</p>
<p>Instead, more preferably, the odds ratio should be calculated as
exponentiating the model parameters. The one unit increment in the
balance results increases the chance of defaulting on by <span
class="math inline">\(0.056\%\)</span> <span
class="math inline">\(((e^{0.00056}= 1.00056) - 1)\)</span>.</p>
<p>The next thing we should understand is how the <code>predict()</code>
function works with <code>glm()</code>. So, lets look at some
predictions.</p>
<pre class="r"><code>head(predict(model_glm))</code></pre>
<pre><code>##      2369      5273      9290      1252      8826       356 
## -5.523309 -5.005989 -5.153738 -4.109757 -6.722855 -6.788078</code></pre>
<p>By default, <code>predict.glm()</code> uses
<code>type = "link"</code>.</p>
<pre class="r"><code>head(predict(model_glm, type = &quot;link&quot;))</code></pre>
<pre><code>##      2369      5273      9290      1252      8826       356 
## -5.523309 -5.005989 -5.153738 -4.109757 -6.722855 -6.788078</code></pre>
<p>That is, <code>R</code> is returning</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  +
\hat{\beta}_p x_p
\]</span></p>
<p>for each observation.</p>
<p>Importantly, these are <strong>not</strong> predicted probabilities.
To obtain the predicted probabilities</p>
<p><span class="math display">\[
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x)
\]</span></p>
<p>we need to use <code>type = "response"</code></p>
<pre class="r"><code>head(predict(model_glm, type = &quot;response&quot;))</code></pre>
<pre><code>##        2369        5273        9290        1252        8826         356 
## 0.003976737 0.006653152 0.005744575 0.016146766 0.001201653 0.001125864</code></pre>
<p>Note that these are probabilities, <strong>not</strong>
classifications. To obtain classifications, we will need to compare to
the correct cutoff value with an <code>ifelse()</code> statement.</p>
<pre class="r"><code>#model_glm_pred = ifelse(predict(model_glm, type = &quot;link&quot;) &gt; 0, &quot;Yes&quot;, &quot;No&quot;)
model_glm_pred = ifelse(predict(model_glm, type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)</code></pre>
<p>The line that is run is performing</p>
<p><span class="math display">\[
\hat{C}(x) =
\begin{cases}
      1 &amp; \hat{p}(x) &gt; 0.5 \\
      0 &amp; \hat{p}(x) \leq 0.5
\end{cases}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x).
\]</span></p>
<p>The commented line, which would give the same results, is
performing</p>
<p><span class="math display">\[
\hat{C}(x) =
\begin{cases}
      1 &amp; \hat{f}(x) &gt; 0 \\
      0 &amp; \hat{f}(x) \leq 0
\end{cases}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{f}(x) =\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 +
\cdots  + \hat{\beta}_p x_p.
\]</span></p>
<p>Once we have classifications, we can calculate metrics such as the
trainging classification error rate.</p>
<pre class="r"><code>calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}</code></pre>
<pre class="r"><code>calc_class_err(actual = default_trn$default, predicted = model_glm_pred)</code></pre>
<pre><code>## [1] 0.027875</code></pre>
<p>As you see, the classification error rate for train data is
approximately 3%, which is OK. In addition to this metric, we should
consider accuracy, sensitivity and specificity which are obtained from
confusion matrix.</p>
<p>One can make a confusion matrix to summarize them</p>
<p><span class="math display">\[
\begin{array}{|c|c|c|}
\hline &amp; \text { Positive True Value } &amp; \text { Negative True
Value } \\
\hline \text { Positive Prediction } &amp; \text { True positive } &amp;
\text { False positive } \\
\hline \text { Negative Prediction } &amp; \text { False negative }
&amp; \text { True negative } \\
\hline
\end{array}
\]</span></p>
<p>Sensitivity, specificity and accuracy are described in terms of TP,
TN, FN and FP.</p>
<p><span class="math display">\[
\textbf{Sensitivity}= \frac{TP}{(TP + FN)} = \frac{\text{(Number of true
positive assessment)}}{\text{(Number of all positive assessment)}}
\]</span></p>
<p><span class="math display">\[
\textbf{Specificity}= \frac{TN}{(TN + FP)} = \frac{\text{(Number of true
negative assessment)}}{\text{(Number of all negative assessment)}}
\]</span></p>
<p><span class="math display">\[
\textbf{Accuracy}=   \frac{(TN + TP)}{(TN+TP+FN+FP)} =
\frac{\text{(Number of correct assessments)}}{\text{(Number of all
assessments)}}
\]</span></p>
<p>As suggested by above equations, sensitivity is the proportion of
true positives that are correctly identified by a diagnostic test. It
shows how good the test is at detecting a disease.</p>
<p>Specificity is the proportion of the true negatives correctly
identified by a diagnostic test. It suggests how good the test is at
identifying normal (negative) condition.</p>
<p>Accuracy is the proportion of true results, either true positive or
true negative, in a population. It measures the degree of veracity of a
diagnostic test on a condition.</p>
<p>We wish to have a model with high sensitivity, specificity and
accuracy. (greater than 0.8) However, this rarely occurs. So, select a
model having two high metrics.</p>
<p>To calculate them in R quickly, the <code>table()</code> and
<code>confusionMatrix()</code> functions can be used.</p>
<pre class="r"><code>train_tab = table(predicted = model_glm_pred, actual = default_trn$default)
library(caret)
train_con_mat = confusionMatrix(train_tab, positive = &quot;Yes&quot;)
c(train_con_mat$overall[&quot;Accuracy&quot;], 
  train_con_mat$byClass[&quot;Sensitivity&quot;], 
  train_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9721250   0.3161765   0.9952122</code></pre>
<p>As you see, we have high accuracy and specificity, but low
sensitivity. However, these are not enough for talking about the
goodness of the model because these metrics are calculated for training
case, not test case.</p>
<p>You can use mosaic plot to visualize your confusion matrix.</p>
<pre class="r"><code>mosaicplot(train_tab)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-43-1.png" width="624" /></p>
<p>This visual clearly shows why the sensivity is low and specifity is
high for our model.</p>
<p>Now, we can calculate them for the test case.</p>
<pre class="r"><code>model_glm_pred_test= ifelse(predict(model_glm, type = &quot;response&quot;,newdata = default_tst) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)
#As you see we use newdata argument that equals to test data</code></pre>
<pre class="r"><code>test_tab = table(predicted = model_glm_pred_test, actual = default_tst$default)
library(caret)
test_con_mat = confusionMatrix(test_tab, positive = &quot;Yes&quot;)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9725000   0.2459016   0.9953584</code></pre>
<p>As you see, we have high accuracy and specificity, but low
sensitivity.</p>
<div id="section-multiple-logistic-regression" class="section level4">
<h4><span style="color:darkred"><strong>Multiple Logistic
Regression</strong></span></h4>
<p>If we include more than one predictors in the model, the way used is
called multiple logistic regression model. Before the model, examine the
relationship between predictors to avoid of multicollinearity
problem.</p>
<pre class="r"><code>library(corrplot)
correlations &lt;- cor(default_trn[,3:4])
corrplot(correlations, method=&quot;circle&quot;)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-46-1.png" width="624" /></p>
<p>As you see, there is no strong association between predictors. That
means there is no mutlicollinearity problem.</p>
<p>When you are dealing with multiple logistic regression, the starting
with visuals are helpful to get an insight about the data.</p>
<p>Let us draw a scatter plot between the numeric variables in the data
which are colored by default status.</p>
<p>To obtain better visuals, we recode our response variable as 0 and
1</p>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 4.1.3</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>default_trn1&lt;-default_trn%&gt;%mutate(default_number = recode(default, &quot;Yes&quot;=1,&quot;No&quot;=0))
head(default_trn1)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["default"],"name":[1],"type":["fct"],"align":["left"]},{"label":["student"],"name":[2],"type":["fct"],"align":["left"]},{"label":["balance"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["income"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["default_number"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"No","2":"No","3":"943.1324","4":"30178.734","5":"0","_rn_":"2369"},{"1":"No","2":"No","3":"1035.4859","4":"41714.374","5":"0","_rn_":"5273"},{"1":"No","2":"Yes","3":"1009.1093","4":"8868.657","5":"0","_rn_":"9290"},{"1":"No","2":"No","3":"1195.4840","4":"38452.641","5":"0","_rn_":"1252"},{"1":"No","2":"Yes","3":"728.9858","4":"17360.523","5":"0","_rn_":"8826"},{"1":"No","2":"No","3":"717.3419","4":"44266.377","5":"0","_rn_":"356"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>library(ggplot2)
ggplot(default_trn1,aes(x=income,y=balance,color=default_number))+geom_point(alpha=0.5)+scale_color_gradient2(midpoint=0.5)+theme_bw()</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-48-1.png" width="624" /></p>
<pre class="r"><code>model_glm_multi = glm(default ~ ., data = default_trn, family = &quot;binomial&quot;)</code></pre>
<p>After fitting the model, display its summary.</p>
<pre class="r"><code>summary(model_glm_multi)</code></pre>
<pre><code>## 
## Call:
## glm(formula = default ~ ., family = &quot;binomial&quot;, data = default_trn)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4691  -0.1403  -0.0547  -0.0199   3.7628  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.086e+01  5.474e-01 -19.840   &lt;2e-16 ***
## studentYes  -6.501e-01  2.618e-01  -2.484    0.013 *  
## balance      5.797e-03  2.593e-04  22.351   &lt;2e-16 ***
## income       1.683e-08  9.132e-06   0.002    0.999    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2374.1  on 7999  degrees of freedom
## Residual deviance: 1257.7  on 7996  degrees of freedom
## AIC: 1265.7
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>We start to check the significance of the model. For this purpose,
the following hypothesis is conducted and null and residual deviance
values are used to test. Then, the difference between null and residual
deviance is compared to chi-square statistic with degrees of freedom 4
because 4 represents the number of parameters that we tested.</p>
<p><span class="math inline">\(H_0\)</span>= <span
class="math inline">\(\beta_0\)</span>=<span
class="math inline">\(\beta_1\)</span>=<span
class="math inline">\(\beta_2\)</span>=<span
class="math inline">\(\beta_3\)</span>=0 vs <span
class="math inline">\(H_1\)</span>= At least one of them is
different.</p>
<pre class="r"><code>LikelihoodRatioTest(model_glm_multi)</code></pre>
<pre><code>## [1] &quot;At least one of the explanatory variables are significant&quot;</code></pre>
<pre class="r"><code>anova(model_glm_multi, test = &quot;Chisq&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Df"],"name":[1],"type":["int"],"align":["right"]},{"label":["Deviance"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Resid. Df"],"name":[3],"type":["int"],"align":["right"]},{"label":["Resid. Dev"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Pr(>Chi)"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"NA","2":"NA","3":"7999","4":"2374.124","5":"NA","_rn_":"NULL"},{"1":"1","2":"1.113108e+01","3":"7998","4":"2362.993","5":"8.489305e-04","_rn_":"student"},{"1":"1","2":"1.105262e+03","3":"7997","4":"1257.731","5":"2.372130e-242","_rn_":"balance"},{"1":"1","2":"3.395725e-06","3":"7996","4":"1257.731","5":"9.985297e-01","_rn_":"income"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>As you see the model is significant on the average.</p>
<p>After checking this, the coefficients are examined. Among three
predictors, all of them contribute model significantly except
income.<br />
Balance has significantly positive effect on the log odds of default
status. The expected change in log odds of default for one unit increase
in the average balance is 0.0058. We can also say that one unit
increment in the balance increases the chance of defaulting on the dept
by <span class="math inline">\(0.58%\)</span> <span
class="math inline">\(((e^{0.00058}= 1.00058)-1)\)</span>.</p>
<p>In addition to this, we can say that the log odds of default is -0.65
times less for student customers compared to other. In other words, we
can say the default status is <span class="math inline">\(48\%\)</span>
less for students compared to others. <span
class="math inline">\(e^{-0.65}= 0.52\)</span> <span
class="math inline">\((1-0.52=0.48=48%)\)</span></p>
<p>Check multicollinearity formally.</p>
<pre class="r"><code>car::vif(model_glm_multi)</code></pre>
<pre><code>##  student  balance   income 
## 2.740601 1.061902 2.663922</code></pre>
<p>It is verified that there is no multicollinearity problem as we
expect. (See above no correlation among predictors.)</p>
<p><strong>Question</strong></p>
<p>Is there any confounding in this problem?</p>
<p>Now, lets calculate the performance metrics for the train data.</p>
<pre class="r"><code>model_glm_pred_train= ifelse(predict(model_glm_multi, type = &quot;response&quot;,newdata = default_trn) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)
#As you see we use newdata argument that equals to test data</code></pre>
<pre class="r"><code>train_tab = table(predicted = model_glm_pred_train, actual = default_trn$default)
library(caret)
test_con_mat = confusionMatrix(train_tab, positive = &quot;Yes&quot;)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9727500   0.3198529   0.9957298</code></pre>
<p>We can visualize the performance of our model on the train data. In
doing so,</p>
<ul>
<li><p>First we create an object <code>default_number</code> that
includes our probabilities obtained from trained data.</p></li>
<li><p>Then, we bind this object with the covariates from the train data
and make it a new data frame.</p></li>
<li><p>Lastly, we add these predicted probabilities to the plot drawn
above by adding second <code>geom_point()</code> function.</p></li>
</ul>
<pre class="r"><code>default_number&lt;-predict(model_glm_multi, type = &quot;response&quot;,newdata = default_trn)
predicted_data&lt;-cbind(default_number,(default_trn[,2:4]))</code></pre>
<pre class="r"><code>library(ggplot2)
ggplot(default_trn1,aes(x=income,y=balance,color=default_number))+geom_point(alpha=0.5)+scale_color_gradient2(midpoint=0.5)+theme_bw()+geom_point(data=predicted_data,size =1,shape=0.3)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-57-1.png" width="624" /></p>
<p>Now, lets calculate the performance metrics for the test data.</p>
<pre class="r"><code>model_glm_pred_test= ifelse(predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)
#As you see we use newdata argument that equals to test data</code></pre>
<p>In this code, 0.5 is the cutoff value generally used in the
literature. As stated before, we estimate the probabilities by using
predict function. Then, we say that if the estimated probability is
greater than the defined cutoff value, then it is coded as 1 or Yes.
Otherwise, it is zero.</p>
<pre class="r"><code>test_tab = table(predicted = model_glm_pred_test, actual = default_tst$default)
library(caret)
test_con_mat = confusionMatrix(test_tab, positive = &quot;Yes&quot;)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9755000   0.3114754   0.9963899</code></pre>
<p>As you see, we have high accuracy and specificity, but low
sensitivity. However, we can say that our model can be OK on the
average.</p>
<p>Here, I wanna mention an important issue, cutoff point.</p>
<p>Consider different cutoff points and see the performance of the
model. In other words, obtain predictions using a low, medium, and high
cutoff. (0.1, 0.5, and 0.9)</p>
<p><strong>For 0.1</strong></p>
<pre class="r"><code>model_glm_pred_test_0.1= ifelse(predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst) &gt; 0.1, &quot;Yes&quot;, &quot;No&quot;)
#As you see we use newdata argument that equals to test data</code></pre>
<pre class="r"><code>test_tab = table(predicted = model_glm_pred_test_0.1, actual = default_tst$default)
library(caret)
test_con_mat = confusionMatrix(test_tab, positive = &quot;Yes&quot;)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9375000   0.7213115   0.9443012</code></pre>
<p><strong>For 0.5</strong></p>
<pre class="r"><code>model_glm_pred_test_0.5= ifelse(predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)
#As you see we use newdata argument that equals to test data</code></pre>
<pre class="r"><code>test_tab = table(predicted = model_glm_pred_test_0.5, actual = default_tst$default)
library(caret)
test_con_mat = confusionMatrix(test_tab, positive = &quot;Yes&quot;)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9755000   0.3114754   0.9963899</code></pre>
<p><strong>For 0.9</strong></p>
<pre class="r"><code>model_glm_pred_test_0.9= ifelse(predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst) &gt; 0.9, &quot;Yes&quot;, &quot;No&quot;)
#As you see we use newdata argument that equals to test data</code></pre>
<pre class="r"><code>test_tab = table(predicted = model_glm_pred_test_0.9, actual = default_tst$default)
library(caret)
test_con_mat = confusionMatrix(test_tab, positive = &quot;Yes&quot;)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])</code></pre>
<pre><code>##    Accuracy Sensitivity Specificity 
##  0.97050000  0.03278689  1.00000000</code></pre>
<p>As you see the model gives its best for the value of 0.1. You can
also try different ways to obtain optimum cutoff value</p>
<pre class="r"><code>prediction&lt;-predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst)
library(InformationValue)</code></pre>
<pre><code>## 
## Attaching package: &#39;InformationValue&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:caret&#39;:
## 
##     confusionMatrix, precision, sensitivity, specificity</code></pre>
<pre class="r"><code>optCutOff &lt;- optimalCutoff(default_tst$default, prediction)[1]
optCutOff</code></pre>
<pre><code>## [1] 0.004064476</code></pre>
<p>As seen best cutoff for our problem is 0.0040645 by the package. Try
this!</p>
<pre><code>model_glm_pred_test_opt= ifelse(predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst) &gt; 0.004, &quot;Yes&quot;, &quot;No&quot;)
test_tab = table(predicted = model_glm_pred_test_opt, actual = default_tst$default)
library(caret)
test_con_mat = confusionMatrix(test_tab)
c(test_con_mat$overall[&quot;Accuracy&quot;], 
  test_con_mat$byClass[&quot;Sensitivity&quot;], 
  test_con_mat$byClass[&quot;Specificity&quot;])

Accuracy Sensitivity Specificity 
0.6385000   0.6276431   0.9836066 </code></pre>
<p><strong>What is AUC - ROC curve?</strong></p>
<p>AUC - ROC curve is a performance measurement for classification
problem at various thresholds settings. ROC is a probability curve and
AUC represents degree or measure of separability. It tells how much
model is capable of distinguishing between classes. Higher the AUC,
better the model is at predicting 0s as 0s and 1s as 1s.</p>
<p>To draw this curve, we can consider <code>pROC</code> package.</p>
<pre class="r"><code>library(pROC)
test_prob=predict(model_glm_multi, type = &quot;response&quot;,newdata = default_tst)
test_roc = roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-67-1.png" width="624" /></p>
<pre class="r"><code>as.numeric(test_roc$auc)</code></pre>
<pre><code>## [1] 0.9340289</code></pre>
<p>A good model will have a high AUC, that is as often as possible a
high sensitivity and specificity.</p>
</div>
<div id="section-multinomial-logistic-regression"
class="section level4">
<h4><span style="color:darkred"><strong>Multinomial Logistic
Regression</strong></span></h4>
<p>What if the response contains more than two categories? For that we
need multinomial logistic regression.</p>
<p><span class="math display">\[
P(Y = k \mid { X = x}) = \frac{e^{\beta_{0k} + \beta_{1k} x_1 + \cdots
+  + \beta_{pk} x_p}}{\sum_{g = 1}^{G} e^{\beta_{0g} + \beta_{1g} x_1 +
\cdots + \beta_{pg} x_p}}
\]</span></p>
<p><strong>Exercise Time</strong></p>
<p>Please click <a
href="https://users.metu.edu.tr/ozancan/r8e2.zip">here</a> and download
your exercise.</p>
</div>
</div>
</div>
<div id="section-robust-statistics" class="section level2">
<h2><span style="color:darkred"><strong>Robust
Statistics</strong></span></h2>
<p>Assume that you are estimating a parameter from a distribution <span
class="math inline">\(f(x)\)</span> using Maximum Likelihood Estimator.
However, the data has actually distribution <span
class="math inline">\(g(x)\)</span>. In this case, you obtain an
estimator which leads to wrong inference.</p>
<p>Suppose that you are interested in fitting a linear regression
because you get the linear relationship between your response and
predictor. However, there is a problem that the response does not have
normal distribution because of the outliers in the sample.</p>
<p>Robust statistics arises as a solution for such cases.</p>
<div id="section-m-estimator" class="section level3">
<h3><span style="color:darkred"><strong>M-Estimator</strong></span></h3>
<p>The M-estimator is a robust regression method often used as an
alternative to the least squares method when data has outliers, extreme
observations, or does not follow a normal distribution.</p>
<p>While the <strong>M</strong> indicates that M estimation is of the
maximum likelihood type (Susanti et. al, 2013), M-estimators are
actually a broad class of estimators that include the maximal likelihood
estimator (Jureckova &amp; Picek, 2005). Least squares estimators and
LAV Estimators are also both special cases of M-estimation (Anderson,
2008).</p>
<p><strong>When to Use the M-Estimator?</strong></p>
<p>The M-estimator is more efficient than Ordinary Least Squares (OLS)
under certain conditions:</p>
<ul>
<li><p>Your data contains y outliers,</p></li>
<li><p>The model matrix X is measured with no errors (Anderson,
2008).</p></li>
</ul>
<p>M-estimators are especially useful when your data has outliers or is
contaminated because one outlier (or heavy tailed errors) can render the
normal-distribution based OLS useless; In that case, you have two
options: remove the badly-behaving outliers, or use the robust
M-estimator.</p>
<p>M-estimation is not recommended when:</p>
<ul>
<li><p>Anomalous data reflects the true population, or</p></li>
<li><p>The population is made up of distinct mixture of distributions
(Little, 2013).</p></li>
</ul>
<p><strong>How M Estimation Works?</strong></p>
<p>M estimation attempts to reduce the influence of outliers by
replacing the squared residuals in OLS by another function of the
residuals:</p>
<p><span class="math inline">\(\min \sum_{i}
\rho\left(\mathrm{r}_{i}\right)\)</span></p>
<p>Most common examples of M estimators</p>
<ul>
<li><p>Trimmed mean</p></li>
<li><p>Huber</p></li>
<li><p>Tukey bisquare</p></li>
<li><p>Hambel</p></li>
</ul>
<p>Robust regression can be used in any situation in which you would use
least squares regression. When fitting a least squares regression, we
might find some outliers or high leverage data points. We have decided
that these data points are not data entry errors, neither they are from
a different population than most of our data. So we have no compelling
reason to exclude them from the analysis. Robust regression might be a
good strategy since it is a compromise between excluding these points
entirely from the analysis and including all the data points and
treating all them equally in OLS regression. The idea of robust
regression is to weigh the observations differently based on how well
behaved these observations are. Roughly speaking, it is a form of
weighted and reweighted least squares regression.</p>
<p><strong>Useful Definitions</strong></p>
<p><strong>Residual:</strong> The difference between the predicted value
(based on the regression equation) and the actual, observed value.</p>
<p><strong>Leverage:</strong> An observation with an extreme value on a
predictor variable is a point with high leverage. Leverage is a measure
of how far an independent variable deviates from its mean. High leverage
points can have a great amount of effect on the estimate of regression
coefficients.</p>
<p><strong>Influence:</strong> An observation is said to be influential
if removing the observation substantially changes the estimate of the
regression coefficients. Influence can be thought of as the product of
leverage and outlierness.</p>
<p><strong>Cooks distance (or Cooks D):</strong> A measure that combines
the information of leverage and residual of the observation.</p>
</div>
<div id="section-application-1" class="section level3">
<h3><span style="color:darkred"><strong>Application</strong></span></h3>
<p>In this example, we will use the crime dataset that appears in
Statistical Methods for Social Sciences, Third Edition by Alan Agresti
and Barbara Finlay (Prentice Hall, 1997). The variables are state id
(sid), state name (state), violent crimes per 100,000 people (crime),
murders per 1,000,000 (murder), the percent of the population living in
metropolitan areas (pctmetro), the percent of the population that is
white (pctwhite), percent of population with a high school education or
above (pcths), percent of population living under poverty line
(poverty), and percent of population that are single parents (single).
It has 51 observations. We are going to use poverty and single to
predict crime.</p>
<pre class="r"><code>cdata&lt;-read.table(&quot;https://users.metu.edu.tr/ozancan/crime.txt&quot;,header = TRUE)
head(cdata)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["sid"],"name":[1],"type":["int"],"align":["right"]},{"label":["state"],"name":[2],"type":["chr"],"align":["left"]},{"label":["crime"],"name":[3],"type":["int"],"align":["right"]},{"label":["murder"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["pctmetro"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["pctwhite"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["pcths"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["poverty"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["single"],"name":[9],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"ak","3":"761","4":"9.0","5":"41.8","6":"75.2","7":"86.6","8":"9.1","9":"14.3","_rn_":"1"},{"1":"2","2":"al","3":"780","4":"11.6","5":"67.4","6":"73.5","7":"66.9","8":"17.4","9":"11.5","_rn_":"2"},{"1":"3","2":"ar","3":"593","4":"10.2","5":"44.7","6":"82.9","7":"66.3","8":"20.0","9":"10.7","_rn_":"3"},{"1":"4","2":"az","3":"715","4":"8.6","5":"84.7","6":"88.6","7":"78.7","8":"15.4","9":"12.1","_rn_":"4"},{"1":"5","2":"ca","3":"1078","4":"13.1","5":"96.7","6":"79.3","7":"76.2","8":"18.2","9":"12.5","_rn_":"5"},{"1":"6","2":"co","3":"567","4":"5.8","5":"81.8","6":"92.5","7":"84.4","8":"9.9","9":"12.1","_rn_":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>summary(cdata)</code></pre>
<pre><code>##       sid          state               crime            murder      
##  Min.   : 1.0   Length:51          Min.   :  82.0   Min.   : 1.600  
##  1st Qu.:13.5   Class :character   1st Qu.: 326.5   1st Qu.: 3.900  
##  Median :26.0   Mode  :character   Median : 515.0   Median : 6.800  
##  Mean   :26.0                      Mean   : 612.8   Mean   : 8.727  
##  3rd Qu.:38.5                      3rd Qu.: 773.0   3rd Qu.:10.350  
##  Max.   :51.0                      Max.   :2922.0   Max.   :78.500  
##     pctmetro         pctwhite         pcths          poverty     
##  Min.   : 24.00   Min.   :31.80   Min.   :64.30   Min.   : 8.00  
##  1st Qu.: 49.55   1st Qu.:79.35   1st Qu.:73.50   1st Qu.:10.70  
##  Median : 69.80   Median :87.60   Median :76.70   Median :13.10  
##  Mean   : 67.39   Mean   :84.12   Mean   :76.22   Mean   :14.26  
##  3rd Qu.: 83.95   3rd Qu.:92.60   3rd Qu.:80.10   3rd Qu.:17.40  
##  Max.   :100.00   Max.   :98.50   Max.   :86.60   Max.   :26.40  
##      single     
##  Min.   : 8.40  
##  1st Qu.:10.05  
##  Median :10.90  
##  Mean   :11.33  
##  3rd Qu.:12.05  
##  Max.   :22.10</code></pre>
<p>In most cases, we begin by running an OLS regression and doing some
diagnostics. We will begin by running an OLS regression and looking at
diagnostic plots examining residuals, fitted values, Cooks distance, and
leverage which help us to detect existence of outlier.</p>
<pre class="r"><code>ols &lt;- lm(crime ~ poverty + single, data = cdata)
summary(ols)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crime ~ poverty + single, data = cdata)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -811.14 -114.27  -22.44  121.86  689.82 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1368.189    187.205  -7.308 2.48e-09 ***
## poverty         6.787      8.989   0.755    0.454    
## single        166.373     19.423   8.566 3.12e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 243.6 on 48 degrees of freedom
## Multiple R-squared:  0.7072, Adjusted R-squared:  0.695 
## F-statistic: 57.96 on 2 and 48 DF,  p-value: 1.578e-13</code></pre>
<pre class="r"><code>opar &lt;- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(ols, las = 1)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-71-1.png" width="624" /></p>
<p>From these plots, we can identify observations 9, 25, and 51 as
possibly problematic to our model. We can look at these observations to
see which states they represent</p>
<pre class="r"><code>cdata[c(9, 25, 51), 1:2]</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["sid"],"name":[1],"type":["int"],"align":["right"]},{"label":["state"],"name":[2],"type":["chr"],"align":["left"]}],"data":[{"1":"9","2":"fl","_rn_":"9"},{"1":"25","2":"ms","_rn_":"25"},{"1":"51","2":"dc","_rn_":"51"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>DC, Florida and Mississippi have either high leverage or large
residuals. We can display the observations that have relatively large
values of Cooks D. A conventional cutoff point is <span
class="math inline">\(4/n\)</span>, where <span
class="math inline">\(n\)</span> is the number of observations in the
data set. We will use this criterion to select the values to
display.</p>
<pre class="r"><code>library(MASS)
d1 &lt;- cooks.distance(ols)
r &lt;- stdres(ols) #MASS package is used
a &lt;- cbind(cdata, d1, r)
a[d1 &gt; 4/51, ]</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["sid"],"name":[1],"type":["int"],"align":["right"]},{"label":["state"],"name":[2],"type":["chr"],"align":["left"]},{"label":["crime"],"name":[3],"type":["int"],"align":["right"]},{"label":["murder"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["pctmetro"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["pctwhite"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["pcths"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["poverty"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["single"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["d1"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["r"],"name":[11],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"ak","3":"761","4":"9.0","5":"41.8","6":"75.2","7":"86.6","8":"9.1","9":"14.3","10":"0.1254750","11":"-1.397418","_rn_":"1"},{"1":"9","2":"fl","3":"1206","4":"8.9","5":"93.0","6":"83.5","7":"74.4","8":"17.8","9":"10.6","10":"0.1425892","11":"2.902663","_rn_":"9"},{"1":"25","2":"ms","3":"434","4":"13.5","5":"30.7","6":"63.3","7":"64.3","8":"24.7","9":"14.7","10":"0.6138721","11":"-3.562990","_rn_":"25"},{"1":"51","2":"dc","3":"2922","4":"78.5","5":"100.0","6":"31.8","7":"73.1","8":"26.4","9":"22.1","10":"2.6362521","11":"2.616447","_rn_":"51"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We probably should drop DC to begin with since it is not even a
state. We include it in the analysis just to show that it has large
Cooks D and demonstrate how it will be handled by <code>rlm</code>. Now
we will look at the residuals. We will generate a new variable called
rabs, which is the absolute value of the residuals (because the sign of
the residual does not matter). We then print the ten observations with
the highest absolute residual values.</p>
<pre class="r"><code>rabs &lt;- abs(r)
a &lt;- cbind(cdata, d1, r, rabs)
asorted &lt;- a[order(-rabs), ]
asorted[1:10, ]</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["sid"],"name":[1],"type":["int"],"align":["right"]},{"label":["state"],"name":[2],"type":["chr"],"align":["left"]},{"label":["crime"],"name":[3],"type":["int"],"align":["right"]},{"label":["murder"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["pctmetro"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["pctwhite"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["pcths"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["poverty"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["single"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["d1"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["r"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["rabs"],"name":[12],"type":["dbl"],"align":["right"]}],"data":[{"1":"25","2":"ms","3":"434","4":"13.5","5":"30.7","6":"63.3","7":"64.3","8":"24.7","9":"14.7","10":"0.61387207","11":"-3.562990","12":"3.562990","_rn_":"25"},{"1":"9","2":"fl","3":"1206","4":"8.9","5":"93.0","6":"83.5","7":"74.4","8":"17.8","9":"10.6","10":"0.14258916","11":"2.902663","12":"2.902663","_rn_":"9"},{"1":"51","2":"dc","3":"2922","4":"78.5","5":"100.0","6":"31.8","7":"73.1","8":"26.4","9":"22.1","10":"2.63625209","11":"2.616447","12":"2.616447","_rn_":"51"},{"1":"46","2":"vt","3":"114","4":"3.6","5":"27.0","6":"98.4","7":"80.8","8":"10.0","9":"11.0","10":"0.04271547","11":"-1.742409","12":"1.742409","_rn_":"46"},{"1":"26","2":"mt","3":"178","4":"3.0","5":"24.0","6":"92.6","7":"81.0","8":"14.9","9":"10.8","10":"0.01675501","11":"-1.460884","12":"1.460884","_rn_":"26"},{"1":"21","2":"me","3":"126","4":"1.6","5":"35.7","6":"98.5","7":"78.8","8":"10.7","9":"10.6","10":"0.02233127","11":"-1.426741","12":"1.426741","_rn_":"21"},{"1":"1","2":"ak","3":"761","4":"9.0","5":"41.8","6":"75.2","7":"86.6","8":"9.1","9":"14.3","10":"0.12547498","11":"-1.397418","12":"1.397418","_rn_":"1"},{"1":"31","2":"nj","3":"627","4":"5.3","5":"100.0","6":"80.8","7":"76.7","8":"10.9","9":"9.6","10":"0.02229185","11":"1.354150","12":"1.354150","_rn_":"31"},{"1":"14","2":"il","3":"960","4":"11.4","5":"84.0","6":"81.0","7":"76.2","8":"13.6","9":"11.5","10":"0.01265689","11":"1.338192","12":"1.338192","_rn_":"14"},{"1":"20","2":"md","3":"998","4":"12.7","5":"92.8","6":"68.9","7":"78.4","8":"9.7","9":"12.0","10":"0.03569622","11":"1.287087","12":"1.287087","_rn_":"20"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Now, lets run our first robust regression. Robust regression is done
by iterated re-weighted least squares (IRLS). The command for running
robust regression is <code>rlm</code> in the MASS package. There are
several weighting functions that can be used for IRLS. We are going to
first use the Huber weights in this example. We will then look at the
final weights created by the IRLS process. This can be very useful.</p>
<pre class="r"><code>rr.huber &lt;- rlm(crime ~ poverty + single, data = cdata)
summary(rr.huber)</code></pre>
<pre><code>## 
## Call: rlm(formula = crime ~ poverty + single, data = cdata)
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -846.09 -125.80  -16.49  119.15  679.94 
## 
## Coefficients:
##             Value      Std. Error t value   
## (Intercept) -1423.0374   167.5899    -8.4912
## poverty         8.8677     8.0467     1.1020
## single        168.9858    17.3878     9.7186
## 
## Residual standard error: 181.8 on 48 degrees of freedom</code></pre>
<pre class="r"><code>hweights &lt;- data.frame(state = cdata$state, resid = rr.huber$resid, weight = rr.huber$w)
hweights2 &lt;- hweights[order(rr.huber$w), ]
hweights2[1:15, ]</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["state"],"name":[1],"type":["chr"],"align":["left"]},{"label":["resid"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["weight"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"ms","2":"-846.08541","3":"0.2889617","_rn_":"25"},{"1":"fl","2":"679.94332","3":"0.3595479","_rn_":"9"},{"1":"vt","2":"-410.48310","3":"0.5955739","_rn_":"46"},{"1":"dc","2":"376.34468","3":"0.6494129","_rn_":"51"},{"1":"mt","2":"-356.13757","3":"0.6864624","_rn_":"26"},{"1":"me","2":"-337.09615","3":"0.7252262","_rn_":"21"},{"1":"nj","2":"331.11610","3":"0.7383574","_rn_":"31"},{"1":"il","2":"319.10036","3":"0.7661167","_rn_":"14"},{"1":"ak","2":"-313.15530","3":"0.7807431","_rn_":"1"},{"1":"md","2":"307.19142","3":"0.7958151","_rn_":"20"},{"1":"ma","2":"291.20811","3":"0.8395172","_rn_":"19"},{"1":"la","2":"-266.95762","3":"0.9159405","_rn_":"18"},{"1":"al","2":"105.40318","3":"1.0000000","_rn_":"2"},{"1":"ar","2":"30.53585","3":"1.0000000","_rn_":"3"},{"1":"az","2":"-43.25293","3":"1.0000000","_rn_":"4"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We can see that roughly, as the absolute residual goes down, the
weight goes up. In other words, cases with a large residuals tend to be
down-weighted. This output shows us that the observation for Mississippi
will be down-weighted the most. Florida will also be substantially
down-weighted. All observations not shown above have a weight of 1. In
OLS regression, all cases have a weight of 1. Hence, the more cases in
the robust regression that have a weight close to one, the closer the
results of the OLS and robust regressions.</p>
<p>Now, we will observe the effect of robust regression visually.
Consider crime and poverty variables from cdata. Although the poverty
does not have a significant effect in <strong>ols</strong>, we know that
poverty and crime have a significant relationship.</p>
<pre class="r"><code>cor.test(cdata$poverty,cdata$crime)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  cdata$poverty and cdata$crime
## t = 4.1449, df = 49, p-value = 0.0001342
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.2721354 0.6884280
## sample estimates:
##      cor 
## 0.509508</code></pre>
<pre class="r"><code>library(ggplot2)
library(gridExtra)
p1&lt;-ggplot(cdata,aes(x=poverty,y=crime))+geom_point()+theme_grey()+ geom_smooth(method = &quot;lm&quot;)+labs(title=&quot;Linear Reg.&quot;)  
p2&lt;-ggplot(cdata,aes(x=poverty,y=crime))+geom_point()+theme_grey()+ geom_smooth(method = &quot;rlm&quot;)+labs(title=&quot;Robust Reg.&quot;)  
grid.arrange(p1,p2,ncol=2)</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-78-1.png" width="624" /></p>
<p>As seen plot, standard error of robust model is lower than the linear
model.</p>
<pre class="r"><code>ggplot(cdata,aes(x=poverty,y=crime))+geom_point()+theme_grey()+ geom_smooth(method = &quot;rlm&quot;,se=F,aes(colour=&quot;rlm&quot;)) + geom_smooth(method = &quot;lm&quot;,se=F,aes(colour=&quot;lm&quot;))+labs(title = &quot;Robust and Linear Reg. Lines&quot; )</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="Recitation8_files/figure-html/unnamed-chunk-79-1.png" width="624" /></p>
<p>As seen, the linear regression line tends to the outlier compared to
robust regression line.</p>
<p>You can change the weight method in Robust Regression. Run the same
model, but using the bisquare weighting function. Again, we can look at
the weights</p>
<pre class="r"><code>rr.bisquare &lt;- rlm(crime ~ poverty + single, data=cdata, psi = psi.bisquare)
summary(rr.bisquare)</code></pre>
<pre><code>## 
## Call: rlm(formula = crime ~ poverty + single, data = cdata, psi = psi.bisquare)
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -905.59 -140.97  -14.98  114.65  668.38 
## 
## Coefficients:
##             Value      Std. Error t value   
## (Intercept) -1535.3339   164.5062    -9.3330
## poverty        11.6903     7.8987     1.4800
## single        175.9303    17.0678    10.3077
## 
## Residual standard error: 202.3 on 48 degrees of freedom</code></pre>
<pre class="r"><code>biweights &lt;- data.frame(state = cdata$state, resid = rr.bisquare$resid, weight = rr.bisquare$w)
biweights2 &lt;- biweights[order(rr.bisquare$w), ]
biweights2[1:15, ]</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["state"],"name":[1],"type":["chr"],"align":["left"]},{"label":["resid"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["weight"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"ms","2":"-905.5932","3":"0.00765244","_rn_":"25"},{"1":"fl","2":"668.3845","3":"0.25287009","_rn_":"9"},{"1":"vt","2":"-402.8031","3":"0.67149521","_rn_":"46"},{"1":"mt","2":"-360.8996","3":"0.73113679","_rn_":"26"},{"1":"nj","2":"345.9781","3":"0.75134743","_rn_":"31"},{"1":"la","2":"-332.6528","3":"0.76893806","_rn_":"18"},{"1":"me","2":"-328.6142","3":"0.77410326","_rn_":"21"},{"1":"ak","2":"-325.8519","3":"0.77766223","_rn_":"1"},{"1":"il","2":"313.1466","3":"0.79365846","_rn_":"14"},{"1":"md","2":"308.7737","3":"0.79906542","_rn_":"20"},{"1":"ma","2":"297.6067","3":"0.81259680","_rn_":"19"},{"1":"dc","2":"260.6488","3":"0.85444167","_rn_":"51"},{"1":"wy","2":"-234.1951","3":"0.88166086","_rn_":"50"},{"1":"ca","2":"201.4407","3":"0.91171392","_rn_":"5"},{"1":"ga","2":"-186.5799","3":"0.92403304","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We can see that the weight given to Mississippi is dramatically lower
using the bisquare weighting function than the Huber weighting function
and the parameter estimates from these two different weighting methods
differ. When comparing the results of a regular OLS regression and a
robust regression, if the results are very different, you will most
likely want to use the results from the robust regression. Large
differences suggest that the model parameters are being highly
influenced by outliers. Different functions have advantages and
drawbacks. Huber weights can have difficulties with severe outliers, and
bisquare weights can have difficulties converging or may yield multiple
solutions.</p>
<p>As you can see, the results from the two analyses are fairly
different, especially with respect to the coefficients of single and the
constant (intercept). While normally we are not interested in the
constant, if you had centered one or both of the predictor variables,
the constant would be useful. On the other hand, you will notice that
poverty is not statistically significant in either analysis, whereas
single is significant in both analyses.</p>
<p><strong>Compare Performance of <code>rlm()</code> with
<code>lm()</code></strong></p>
<p>Lets build the equivalent lm() model so we can compare the errors
against the respective fitted values.</p>
</div>
<div id="section-calculate-the-errors" class="section level3 tabset">
<h3 class="tabset">Calculate the Errors</h3>
<div id="section-linear-regression" class="section level4">
<h4>Linear Regression</h4>
<pre class="r"><code>library(DMwR)
regr.eval(cdata$crime, ols$fitted.values)</code></pre>
<pre><code>##          mae          mse         rmse         mape 
## 1.756703e+02 5.585495e+04 2.363365e+02 4.381456e-01</code></pre>
</div>
<div id="section-huber" class="section level4">
<h4>Huber</h4>
<pre class="r"><code>regr.eval(cdata$crime, rr.huber$fitted.values)</code></pre>
<pre><code>##          mae          mse         rmse         mape 
## 1.750825e+02 5.605055e+04 2.367500e+02 4.367968e-01</code></pre>
</div>
<div id="section-bisquare" class="section level4">
<h4>BiSquare</h4>
<pre class="r"><code>regr.eval(cdata$crime, rr.bisquare$fitted.values)</code></pre>
<pre><code>##          mae          mse         rmse         mape 
## 1.750399e+02 5.736477e+04 2.395094e+02 4.369935e-01</code></pre>
<p>As expected, the errors from the robust regression model (Huber) is
lesser than the linear regression model. (Based on MAPE)</p>
<p><strong>References:</strong></p>
<ul>
<li><a href="http://uc-r.github.io/logistic_regression"
class="uri">http://uc-r.github.io/logistic_regression</a></li>
<li><a href="https://stats.oarc.ucla.edu/r/dae/robust-regression/"
class="uri">https://stats.oarc.ucla.edu/r/dae/robust-regression/</a></li>
</ul>
<p>
<script type="application/shiny-prerendered" data-context="server-start">
library(learnr)
knitr::opts_chunk$set(echo = TRUE)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>


<script type="application/shiny-prerendered" data-context="server">
learnr:::prepare_tutorial_state(session)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::i18n_observe_tutorial_language(input, session)
</script>


<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::event_trigger(session, "session_stop")
      })
</script>
</p>
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["header-attrs"]},{"type":"character","attributes":{},"value":["2.20"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pandoc"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["header-attrs.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["3.6.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/3.6.0"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery-3.6.0.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquerylib"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.1.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"character","attributes":{},"value":["<style>h1 {font-size: 34px;}\n       h1.title {font-size: 38px;}\n       h2 {font-size: 30px;}\n       h3 {font-size: 24px;}\n       h4 {font-size: 18px;}\n       h5 {font-size: 16px;}\n       h6 {font-size: 12px;}\n       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}\n       pre:not([class]) { background-color: white }<\/style>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.11.2"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.11.2"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["i18n"]},{"type":"character","attributes":{},"value":["21.6.10"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/i18n"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["i18next.min.js","tutorial-i18n-init.js"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["<script id=\"i18n-cstm-trns\" type=\"application/json\">{\"language\":\"en\",\"resources\":{\"en\":{\"translation\":{\"button\":{\"runcode\":\"Run Code\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Hint\",\"hint_plural\":\"Hints\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Next Hint\",\"hintprev\":\"Previous Hint\",\"solution\":\"Solution\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Copy to Clipboard\",\"startover\":\"Start Over\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Continue\",\"submitanswer\":\"Submit Answer\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Previous Topic\",\"nexttopic\":\"Next Topic\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Try Again\"},\"text\":{\"startover\":\"Start Over\",\"areyousure\":\"Are you sure you want to start over? (all exercise progress will be reset)\",\"youmustcomplete\":\"You must complete the\",\"exercise\":\"exercise\",\"exercise_plural\":\"exercises\",\"inthissection\":\"in this section before continuing.\",\"code\":\"Code\",\"enginecap\":\"{{engine}} $t(text.code)\",\"quiz\":\"Quiz\",\"blank\":\"blank\",\"blank_plural\":\"blanks\",\"exercisecontainsblank\":\"This exercise contains {{count}} $t(text.blank).\",\"pleasereplaceblank\":\"Please replace {{blank}} with valid code.\",\"unparsable\":\"It looks like this might not be valid R code. R cannot determine how to turn your text into a complete command. You may have forgotten to fill in a blank, to remove an underscore, to include a comma between arguments, or to close an opening <code>&quot;<\\/code>, <code>'<\\/code>, <code>(<\\/code> or <code>{<\\/code> with a matching <code>&quot;<\\/code>, <code>'<\\/code>, <code>)<\\/code> or <code>}<\\/code>.\\n\",\"unparsablequotes\":\"<p>It looks like your R code contains specially formatted quotation marks or &quot;curly&quot; quotes (<code>{{character}}<\\/code>) around character strings, making your code invalid. R requires character values to be contained in straight quotation marks (<code>&quot;<\\/code> or <code>'<\\/code>).<\\/p> {{code}} <p>Don't worry, this is a common source of errors when you copy code from another app that applies its own formatting to text. You can try replacing the code on that line with the following. There may be other places that need to be fixed, too.<\\/p> {{suggestion}}\\n\",\"unparsableunicode\":\"<p>It looks like your R code contains an unexpected special character (<code>{{character}}<\\/code>) that makes your code invalid.<\\/p> {{code}} <p>Sometimes your code may contain a special character that looks like a regular character, especially if you copy and paste the code from another app. Try deleting the special character from your code and retyping it manually.<\\/p>\\n\",\"unparsableunicodesuggestion\":\"<p>It looks like your R code contains an unexpected special character (<code>{{character}}<\\/code>) that makes your code invalid.<\\/p> {{code}} <p>Sometimes your code may contain a special character that looks like a regular character, especially if you copy and paste the code from another app. You can try replacing the code on that line with the following. There may be other places that need to be fixed, too.<\\/p> {{suggestion}}\\n\",\"and\":\"and\",\"or\":\"or\",\"listcomma\":\", \",\"oxfordcomma\":\",\"}}},\"fr\":{\"translation\":{\"button\":{\"runcode\":\"Lancer le Code\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Indication\",\"hint_plural\":\"Indications\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Indication Suivante\",\"hintprev\":\"Indication Précédente\",\"solution\":\"Solution\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Copier dans le Presse-papier\",\"startover\":\"Recommencer\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Continuer\",\"submitanswer\":\"Soumettre\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Chapitre Précédent\",\"nexttopic\":\"Chapitre Suivant\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Réessayer\"},\"text\":{\"startover\":\"Recommencer\",\"areyousure\":\"Êtes-vous certains de vouloir recommencer? (La progression sera remise à zéro)\",\"youmustcomplete\":\"Vous devez d'abord compléter\",\"exercise\":\"l'exercice\",\"exercise_plural\":\"des exercices\",\"inthissection\":\"de cette section avec de continuer.\",\"code\":\"Code\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Quiz\",\"and\":\"et\",\"or\":\"ou\",\"oxfordcomma\":\"\"}}},\"es\":{\"translation\":{\"button\":{\"runcode\":\"Ejecutar código\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Pista\",\"hint_plural\":\"Pistas\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Siguiente pista\",\"hintprev\":\"Pista anterior\",\"solution\":\"Solución\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Copiar al portapapeles\",\"startover\":\"Reiniciar\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Continuar\",\"submitanswer\":\"Enviar respuesta\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Tema anterior\",\"nexttopic\":\"Tema siguiente\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Volver a intentar\"},\"text\":{\"startover\":\"Reiniciar\",\"areyousure\":\"¿De verdad quieres empezar de nuevo? (todo el progreso del ejercicio se perderá)\",\"youmustcomplete\":\"Debes completar\",\"exercise\":\"el ejercicio\",\"exercise_plural\":\"los ejercicios\",\"inthissection\":\"en esta sección antes de continuar.\",\"code\":\"Código\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Cuestionario\",\"and\":\"y\",\"or\":\"o\",\"oxfordcomma\":\"\"}}},\"pt\":{\"translation\":{\"button\":{\"runcode\":\"Executar código\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Dica\",\"hint_plural\":\"Dicas\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Próxima dica\",\"hintprev\":\"Dica anterior\",\"solution\":\"Solução\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Copiar para a área de transferência\",\"startover\":\"Reiniciar\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Continuar\",\"submitanswer\":\"Enviar resposta\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Tópico anterior\",\"nexttopic\":\"Próximo tópico\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Tentar novamente\"},\"text\":{\"startover\":\"Reiniciar\",\"areyousure\":\"Tem certeza que deseja começar novamente? (todo o progresso feito será perdido)\",\"youmustcomplete\":\"Você deve completar\",\"exercise\":\"o exercício\",\"exercise_plural\":\"os exercícios\",\"inthissection\":\"nesta seção antes de continuar.\",\"code\":\"Código\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Quiz\",\"and\":\"e\",\"or\":\"ou\",\"oxfordcomma\":\"\"}}},\"tr\":{\"translation\":{\"button\":{\"runcode\":\"Çalıştırma Kodu\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Ipucu\",\"hint_plural\":\"İpuçları\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Sonraki İpucu\",\"hintprev\":\"Önceki İpucu\",\"solution\":\"Çözüm\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Pano'ya Kopyala\",\"startover\":\"Baştan Başlamak\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Devam et\",\"submitanswer\":\"Cevabı onayla\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Önceki Konu\",\"nexttopic\":\"Sonraki Konu\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Tekrar Deneyin\"},\"text\":{\"startover\":\"Baştan Başlamak\",\"areyousure\":\"Baştan başlamak istediğinizden emin misiniz? (tüm egzersiz ilerlemesi kaybolacak)\",\"youmustcomplete\":\"Tamamlamalısın\",\"exercise\":\"egzersiz\",\"exercise_plural\":\"egzersizler\",\"inthissection\":\"devam etmeden önce bu bölümde\",\"code\":\"Kod\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Sınav\",\"oxfordcomma\":\"\"}}},\"emo\":{\"translation\":{\"button\":{\"runcode\":\"<U+0001F3C3>\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"<U+0001F4A1>\",\"hint_plural\":\"$t(button.hint)\",\"hinttitle\":\"$t(button.hint)\",\"solution\":\"<U+0001F3AF>\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"<U+0001F4CB>\",\"startover\":\"<U+23EE>\",\"startovertitle\":\"Start Over\",\"continue\":\"<U+2705>\",\"submitanswer\":\"<U+0001F197>\",\"submitanswertitle\":\"Submit Answer\",\"previoustopic\":\"<U+2B05>\",\"nexttopic\":\"<U+27A1>\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"<U+0001F501>\"},\"text\":{\"startover\":\"<U+23EE>\",\"areyousure\":\"<U+0001F914>\",\"youmustcomplete\":\"<U+26A0><U+FE0F> <U+0001F449> <U+0001F9D1><U+200D><U+0001F4BB>\",\"exercise\":\"\",\"exercise_plural\":\"\",\"inthissection\":\"\",\"code\":\"<U+0001F4BB>\",\"enginecap\":\"$t(text.code) {{engine}}\",\"oxfordcomma\":\"\"}}},\"eu\":{\"translation\":{\"button\":{\"runcode\":\"Kodea egikaritu\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Laguntza\",\"hint_plural\":\"Laguntza\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Aurreko laguntza\",\"hintprev\":\"Hurrengo laguntza\",\"solution\":\"Ebazpena\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Arbelean kopiatu\",\"startover\":\"Berrabiarazi\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Jarraitu\",\"submitanswer\":\"Erantzuna bidali\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Aurreko atala\",\"nexttopic\":\"Hurrengo atala\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Berriro saiatu\"},\"text\":{\"startover\":\"Berrabiarazi\",\"areyousure\":\"Berriro hasi nahi duzu? (egindako lana galdu egingo da)\",\"youmustcomplete\":\"Aurrera egin baino lehen atal honetako\",\"exercise\":\"ariketa egin behar duzu.\",\"exercise_plural\":\"ariketak egin behar dituzu.\",\"inthissection\":\"\",\"code\":\"Kodea\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Galdetegia\",\"oxfordcomma\":\"\"}}},\"de\":{\"translation\":{\"button\":{\"runcode\":\"Code ausführen\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Tipp\",\"hint_plural\":\"Tipps\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Nächster Tipp\",\"hintprev\":\"Vorheriger Tipp\",\"solution\":\"Lösung\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"In die Zwischenablage kopieren\",\"startover\":\"Neustart\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Weiter\",\"submitanswer\":\"Antwort einreichen\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Vorheriges Kapitel\",\"nexttopic\":\"Nächstes Kapitel\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Nochmal versuchen\"},\"text\":{\"startover\":\"Neustart\",\"areyousure\":\"Bist du sicher, dass du neustarten willst? (der gesamte Lernfortschritt wird gelöscht)\",\"youmustcomplete\":\"Vervollstädinge\",\"exercise\":\"die Übung\",\"exercise_plural\":\"die Übungen\",\"inthissection\":\"in diesem Kapitel, bevor du fortfährst.\",\"code\":\"Code\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Quiz\",\"blank\":\"Lücke\",\"blank_plural\":\"Lücken\",\"pleasereplaceblank\":\"Bitte ersetze {{blank}} mit gültigem Code.\",\"unparsable\":\"Dies scheint kein gültiger R Code zu sein. R kann deinen Text nicht in einen gültigen Befehl übersetzen. Du hast vielleicht vergessen, die Lücke zu füllen, einen Unterstrich zu entfernen, ein Komma zwischen Argumente zu setzen oder ein eröffnendes <code>&quot;<\\/code>, <code>'<\\/code>, <code>(<\\/code> oder <code>{<\\/code> mit einem zugehörigen <code>&quot;<\\/code>, <code>'<\\/code>, <code>)<\\/code> oder <code>}<\\/code> zu schließen.\\n\",\"and\":\"und\",\"or\":\"oder\",\"listcomma\":\", \",\"oxfordcomma\":\",\"}}},\"ko\":{\"translation\":{\"button\":{\"runcode\":\"<U+CF54><U+B4DC> <U+C2E4><U+D589>\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"<U+D78C><U+D2B8>\",\"hint_plural\":\"<U+D78C><U+D2B8><U+B4E4>\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"<U+B2E4><U+C74C> <U+D78C><U+D2B8>\",\"hintprev\":\"<U+C774><U+C804> <U+D78C><U+D2B8>\",\"solution\":\"<U+C194><U+B8E8><U+C158>\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"<U+D074><U+B9BD><U+BCF4><U+B4DC><U+C5D0> <U+BCF5><U+C0AC>\",\"startover\":\"<U+C7AC><U+D559><U+C2B5>\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"<U+B2E4><U+C74C> <U+D559><U+C2B5><U+C73C><U+B85C>\",\"submitanswer\":\"<U+C815><U+B2F5> <U+C81C><U+CD9C>\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"<U+C774><U+C804> <U+D1A0><U+D53D>\",\"nexttopic\":\"<U+B2E4><U+C74C> <U+D1A0><U+D53D>\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"<U+C7AC><U+C2DC><U+B3C4>\"},\"text\":{\"startover\":\"<U+C7AC><U+D559><U+C2B5>\",\"areyousure\":\"<U+B2E4><U+C2DC> <U+C2DC><U+C791> <U+D558><U+C2DC><U+ACA0><U+C2B5><U+B2C8><U+AE4C>? (<U+BAA8><U+B4E0> <U+C608><U+C81C><U+C758> <U+C9C4><U+D589> <U+C815><U+BCF4><U+AC00> <U+C7AC><U+C124><U+C815><U+B429><U+B2C8><U+B2E4>)\",\"youmustcomplete\":\"<U+B2F9><U+C2E0><U+C740> <U+C644><U+B8CC><U+D574><U+C57C> <U+D569><U+B2C8><U+B2E4>\",\"exercise\":\"<U+C5F0><U+C2B5><U+BB38><U+C81C>\",\"exercise_plural\":\"<U+C5F0><U+C2B5><U+BB38><U+C81C><U+B4E4>\",\"inthissection\":\"<U+C774> <U+C139><U+C158><U+C744> <U+C2E4><U+D589><U+D558><U+AE30> <U+C804><U+C5D0>\",\"code\":\"<U+CF54><U+B4DC>\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"<U+D034><U+C988>\",\"blank\":\"<U+ACF5><U+BC31>\",\"blank_plural\":\"<U+ACF5><U+BC31><U+B4E4>\",\"exercisecontainsblank\":\"<U+C774> <U+C5F0><U+C2B5><U+BB38><U+C81C><U+C5D0><U+B294> {{count}}<U+AC1C><U+C758> $t(text.blank)<U+C774> <U+D3EC><U+D568><U+B418><U+C5B4> <U+C788><U+C2B5><U+B2C8><U+B2E4>.\",\"pleasereplaceblank\":\"{{blank}}<U+B97C> <U+C720><U+D6A8><U+D55C> <U+CF54><U+B4DC><U+B85C> <U+BC14><U+AFB8><U+C2ED><U+C2DC><U+C624>.\",\"unparsable\":\"<U+C774><U+AC83><U+C740> <U+C720><U+D6A8><U+D55C> R <U+CF54><U+B4DC><U+AC00> <U+C544><U+B2D0> <U+C218> <U+C788><U+C2B5><U+B2C8><U+B2E4>. R<U+C740> <U+D14D><U+C2A4><U+D2B8><U+B97C> <U+C644><U+C804><U+D55C> <U+BA85><U+B839><U+C73C><U+B85C> <U+BCC0><U+D658><U+D558><U+B294> <U+BC29><U+BC95><U+C744> <U+ACB0><U+C815><U+D560> <U+C218> <U+C5C6><U+C2B5><U+B2C8><U+B2E4>. <U+B2F9><U+C2E0><U+C740> <U+ACF5><U+BC31><U+C774><U+B098> <U+BC11><U+C904><U+C744> <U+B300><U+CCB4><U+D558><U+C5EC> <U+CC44><U+C6B0><U+AE30>, <U+C778><U+C218><U+B97C> <U+CEF4><U+B9C8><U+B85C> <U+AD6C><U+BD84><U+D558><U+AE30>, <U+B610><U+B294> <code>&quot;<\\/code>, <code>'<\\/code>, <code>(<\\/code> , <code>{<\\/code><U+B85C> <U+C2DC><U+C791><U+D558><U+B294> <U+AD6C><U+BB38><U+C744> <U+B2EB><U+B294> <code>&quot;<\\/code>, <code>'<\\/code>, <code>)<\\/code>, <code>}<\\/code><U+C744> <U+C78A><U+C5C8><U+C744> <U+C218><U+B3C4> <U+C788><U+C2B5><U+B2C8><U+B2E4>.\\n\",\"and\":\"<U+ADF8><U+B9AC><U+ACE0>\",\"or\":\"<U+D639><U+C740>\",\"listcomma\":\", \",\"oxfordcomma\":\"\"}}},\"zh\":{\"translation\":{\"button\":{\"runcode\":\"<U+8FD0><U+884C><U+4EE3><U+7801>\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"<U+63D0><U+793A>\",\"hint_plural\":\"<U+63D0><U+793A>\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"<U+4E0B><U+4E00><U+4E2A><U+63D0><U+793A>\",\"hintprev\":\"<U+4E0A><U+4E00><U+4E2A><U+63D0><U+793A>\",\"solution\":\"<U+7B54><U+6848>\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"<U+590D><U+5236><U+5230><U+526A><U+5207><U+677F>\",\"startover\":\"<U+91CD><U+65B0><U+5F00><U+59CB>\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"<U+7EE7><U+7EED>\",\"submitanswer\":\"<U+63D0><U+4EA4><U+7B54><U+6848>\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"<U+4E0A><U+4E00><U+4E13><U+9898>\",\"nexttopic\":\"<U+4E0B><U+4E00><U+4E13><U+9898>\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"<U+518D><U+8BD5><U+4E00><U+6B21>\"},\"text\":{\"startover\":\"<U+91CD><U+7F6E>\",\"areyousure\":\"<U+4F60><U+786E><U+5B9A><U+8981><U+91CD><U+65B0><U+5F00><U+59CB><U+5417>? (<U+6240><U+6709><U+5F53><U+524D><U+8FDB><U+5EA6><U+5C06><U+88AB><U+91CD><U+7F6E>)\",\"youmustcomplete\":\"<U+4F60><U+5FC5><U+987B><U+5B8C><U+6210>\",\"exercise\":\"<U+7EC3><U+4E60>\",\"exercise_plural\":\"<U+7EC3><U+4E60>\",\"inthissection\":\"<U+5728><U+8FDB><U+884C><U+672C><U+8282><U+4E4B><U+524D>\",\"code\":\"<U+4EE3><U+7801>\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"<U+6D4B><U+8BD5>\",\"blank\":\"<U+7A7A>\",\"blank_plural\":\"<U+7A7A>\",\"exercisecontainsblank\":\"<U+672C><U+7EC3><U+4E60><U+5305><U+542B>{{count}}<U+4E2A>$t(text.blank)\",\"pleasereplaceblank\":\"<U+8BF7><U+5728>{{blank}}<U+5185><U+586B><U+5199><U+6070><U+5F53><U+7684><U+4EE3><U+7801>\",\"unparsable\":\"<U+8FD9><U+4F3C><U+4E4E><U+4E0D><U+662F><U+6709><U+6548><U+7684>R<U+4EE3><U+7801><U+3002> R<U+4E0D><U+77E5><U+9053><U+5982><U+4F55><U+5C06><U+60A8><U+7684><U+6587><U+672C><U+8F6C><U+6362><U+4E3A><U+5B8C><U+6574><U+7684><U+547D><U+4EE4><U+3002> <U+60A8><U+662F><U+5426><U+5FD8><U+4E86><U+586B><U+7A7A>,<U+5FD8><U+4E86><U+5220><U+9664><U+4E0B><U+5212><U+7EBF>,<U+5FD8><U+4E86><U+5728><U+53C2><U+6570><U+4E4B><U+95F4><U+5305><U+542B><U+9017><U+53F7>,<U+6216><U+8005><U+662F><U+5FD8><U+4E86><U+7528><code>&quot;<\\/code>, <code>'<\\/code>, <code>)<\\/code>,<code>}<\\/code><U+6765><U+5C01><U+95ED><code>&quot;<\\/code>, <code>'<\\/code>, <code>(<\\/code><U+3002> or <code>{<\\/code><U+3002>\\n\",\"unparsablequotes\":\"<p><U+60A8><U+7684>R<U+4EE3><U+7801><U+4E2D><U+4F3C><U+4E4E><U+542B><U+6709><U+7279><U+6B8A><U+683C><U+5F0F><U+7684><U+5F15><U+53F7>,<U+6216><U+8005><U+5F2F><U+5F15><U+53F7>(<code>{{character}}<\\/code>) <U+5728><U+5B57><U+7B26><U+4E32><U+524D><U+540E>,<U+5728>R<U+4E2D><U+5B57><U+7B26><U+4E32><U+5E94><U+8BE5><U+88AB><U+76F4><U+5F15><U+53F7>(<code>&quot;<\\/code> <U+6216><U+8005> <code>'<\\/code>)<U+5305><U+88F9><U+3002><\\/p> {{code}} <p><U+522B><U+62C5><U+5FC3>,<U+8BE5><U+9519><U+8BEF><U+7ECF><U+5E38><U+5728><U+590D><U+5236><U+7C98><U+8D34><U+5305><U+542B><U+683C><U+5F0F><U+7684><U+4EE3><U+7801><U+65F6><U+9047><U+5230>, <U+60A8><U+53EF><U+4EE5><U+5C1D><U+8BD5><U+5C06><U+8BE5><U+884C><U+4E2D><U+7684><U+4EE3><U+7801><U+66FF><U+6362><U+4E3A><U+4EE5><U+4E0B><U+4EE3><U+7801>,<U+4E5F><U+8BB8><U+8FD8><U+6709><U+5176><U+4ED6><U+5730><U+65B9><U+9700><U+8981><U+4FEE><U+6539><U+3002><\\/p> {{suggestion}}\\n\",\"unparsableunicode\":\"<p><U+60A8><U+7684><U+4EE3><U+7801><U+4E2D><U+4F3C><U+4E4E><U+5305><U+542B><U+6709><U+5F02><U+5E38><U+5B57><U+7B26>(<code>{{character}}<\\/code>),<U+5BFC><U+81F4><U+4EE3><U+7801><U+65E0><U+6548><U+3002><\\/p> {{code}} <p><U+6709><U+65F6><U+5019><U+4F60><U+7684><U+4EE3><U+7801><U+53EF><U+80FD><U+542B><U+6709><U+770B><U+4F3C><U+6B63><U+5E38><U+5B57><U+7B26><U+7684><U+7279><U+6B8A><U+5B57><U+7B26>,<U+7279><U+522B><U+662F><U+5F53><U+4F60><U+590D><U+5236><U+7C98><U+8D34><U+5176><U+4ED6><U+6765><U+6E90><U+4EE3><U+7801><U+7684><U+65F6><U+5019><U+3002> <U+8BF7><U+8BD5><U+7740><U+5220><U+9664><U+8FD9><U+4E9B><U+7279><U+6B8A><U+5B57><U+7B26>,<U+91CD><U+65B0><U+8F93><U+5165><\\/p>\\n\",\"unparsableunicodesuggestion\":\"<p><U+60A8><U+7684><U+4EE3><U+7801><U+4E2D><U+4F3C><U+4E4E><U+5305><U+542B><U+6709><U+5F02><U+5E38><U+5B57><U+7B26>(<code>{{character}}<\\/code>),<U+5BFC><U+81F4><U+4EE3><U+7801><U+65E0><U+6548><U+3002><\\/p> {{code}} <p><U+6709><U+65F6><U+5019><U+4F60><U+7684><U+4EE3><U+7801><U+53EF><U+80FD><U+542B><U+6709><U+770B><U+4F3C><U+6B63><U+5E38><U+5B57><U+7B26><U+7684><U+7279><U+6B8A><U+5B57><U+7B26>,<U+7279><U+522B><U+662F><U+5F53><U+4F60><U+590D><U+5236><U+7C98><U+8D34><U+5176><U+4ED6><U+6765><U+6E90><U+4EE3><U+7801><U+7684><U+65F6><U+5019><U+3002> <U+8BF7><U+8BD5><U+7740><U+5220><U+9664><U+8FD9><U+4E9B><U+7279><U+6B8A><U+5B57><U+7B26>,<U+91CD><U+65B0><U+8F93><U+5165><\\/p>\\n\",\"and\":\"<U+4E14>\",\"or\":\"<U+6216>\",\"listcomma\":\",\",\"oxfordcomma\":\",\"}}},\"pl\":{\"translation\":{\"button\":{\"runcode\":\"Uruchom kod\",\"runcodetitle\":\"$t(button.runcode) ({{kbd}})\",\"hint\":\"Podpowiedz\",\"hint_plural\":\"Podpowiedzi\",\"hinttitle\":\"$t(button.hint)\",\"hintnext\":\"Nastepna podpowiedz\",\"hintprev\":\"Poprzednia podpowiedz\",\"solution\":\"Rozwiazanie\",\"solutiontitle\":\"$t(button.solution)\",\"copyclipboard\":\"Kopiuj do schowka\",\"startover\":\"Zacznij od poczatku\",\"startovertitle\":\"$t(button.startover)\",\"continue\":\"Kontynuuj\",\"submitanswer\":\"Wyslij\",\"submitanswertitle\":\"$t(button.submitanswer)\",\"previoustopic\":\"Poprzednia sekcja\",\"nexttopic\":\"Nastepna sekcja\",\"questionsubmit\":\"$t(button.submitanswer)\",\"questiontryagain\":\"Spróbuj ponownie\"},\"text\":{\"startover\":\"Zacznij od poczatku\",\"areyousure\":\"Czy na pewno chcesz zaczac od poczatku? (caly postep w zadaniu zostanie utracony)\",\"youmustcomplete\":\"Musisz ukonczyc\",\"exercise\":\"cwiczenie\",\"exercise_plural\":\"cwiczenia\",\"inthissection\":\"w tej sekcji przed kontynuowaniem\",\"code\":\"Kod\",\"enginecap\":\"$t(text.code) {{engine}}\",\"quiz\":\"Quiz\",\"blank\":\"luka\",\"blank_plural\":\"luk(i)\",\"exercisecontainsblank\":\"To cwiczenie zawiera {{count}} $t(text.blank).\",\"pleasereplaceblank\":\"Prosze uzupelnic {{blank}} prawidlowym kodem.\",\"unparsable\":\"Wyglada na to, ze moze to nie byc prawidlowy kod R. R nie jest w stanie przetworzyc Twojego tekstu na polecenie. Mogles(-as) zapomniec wypelnic luki, usunac podkreslnik, umiescic przecinka miedzy argumentami, lub zamknac znak <code>&quot;<\\/code>, <code>'<\\/code>, <code>(<\\/code> lub <code>{<\\/code> odpowiadajacym <code>&quot;<\\/code>, <code>'<\\/code>, <code>)<\\/code> lub <code>}<\\/code>.\\n\",\"unparsablequotes\":\"<p>Wyglada na to, ze Twój kod zawiera szczególnie sformatowane cudzyslowy lub cudzyslowy typograficzne (<code>{{character}}<\\/code>) przy ciagach znaków, co sprawia, ze kod jest niepoprawny. R wymaga cudzyslowów prostych (<code>&quot;<\\/code> albo <code>'<\\/code>).<\\/p> {{code}} <p>Nie martw sie, to powszechne zródlo bledów, gdy kopiuje sie kod z innego programu, który sam formatuje teskt. Mozesz spróbowac zastapic swój kod nastepujacym kodem. Moga byc tez inne miejsca, które wymagaja poprawienia.<\\/p> {{suggestion}}\\n\",\"unparsableunicode\":\"<p>Wyglada na to, ze Twój kod zawiera niespodziewany znak specjalny (<code>{{character}}<\\/code>), co sprawia, ze kod jest niepoprawny.<\\/p> {{code}} <p>Czasami Twój kod moze zawierac znak specjalny, który wyglada jak zwykly znak, zwlaszcza jesli kopiujesz kod z innego programu. Spróbuj usunac znak specjalny i wpisac do ponownie recznie.<\\/p>\\n\",\"unparsableunicodesuggestion\":\"<p>Wyglada na to, ze Twój kod zawiera niespodziewany znak specjalny (<code>{{character}}<\\/code>), co sprawia, ze kod jest niepoprawny.<\\/p> {{code}} <p>Czasami Twój kod moze zawierac znak specjalny, który wyglada jak zwykly znak, zwlaszcza jesli kopiujesz kod z innego programu. Mozesz spróbowac zastapic swój kod nastepujacym kodem. Moga byc tez inne miejsca, które wymagaja poprawienia.<\\/p> {{suggestion}}\\n\",\"and\":\"i\",\"or\":\"lub\",\"listcomma\":\", \",\"oxfordcomma\":\"\"}}}}}<\/script>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.11.2"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.11.2"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.11.2"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["3.6.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/3.6.0"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery-3.6.0.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquerylib"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.1.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["3.6.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/3.6.0"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery-3.6.0.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquerylib"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.1.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.20"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["5.5.2"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.11.2"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.11.2"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.11.2"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.11.2"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134]}},"value":[{"type":"character","attributes":{},"value":["abind","admisc","backports","base","bslib","cachem","car","carData","caret","checkmate","class","cli","codetools","colorspace","compiler","corrplot","curl","data.table","datasets","digest","DMwR","dplyr","e1071","ellipsis","epitools","evaluate","expm","fansi","farver","fastmap","foreach","future","future.apply","generics","ggmosaic","ggplot2","ggrepel","globals","glue","gower","graphics","grDevices","grid","gridExtra","gtable","hardhat","highr","htmltools","htmlwidgets","httpuv","httr","InformationValue","ipred","ISLR","iterators","jquerylib","jsonlite","knitr","labeling","later","lattice","lava","lazyeval","learnr","lifecycle","listenv","lmtest","ltm","lubridate","magrittr","MASS","Matrix","methods","mgcv","mime","mnormt","ModelMetrics","msm","munsell","mvtnorm","nlme","nnet","parallel","parallelly","pillar","pkgconfig","plotly","plyr","polycor","pROC","prodlim","productplots","promises","proxy","psych","purrr","quantmod","R6","Rcpp","recipes","reshape2","rlang","rmarkdown","ROCR","rpart","rprojroot","rstudioapi","sass","scales","shiny","splines","stats","stats4","stringi","stringr","survival","tibble","tidyr","tidyselect","timechange","timeDate","tools","TTR","utf8","utils","vcd","vctrs","viridisLite","withr","xfun","xtable","xts","yaml","zoo"]},{"type":"character","attributes":{},"value":["1.4-5","0.30","1.4.1","4.1.0","0.4.2","1.0.6","3.1-1","3.0-5","6.0-93","2.1.0","7.3-21","3.4.1","0.2-19","2.0-2","4.1.0","0.92","5.0.0","1.14.8","4.1.0","0.6.31","0.4.1","1.1.0","1.7-13","0.3.2","0.5-10.1","0.20","0.999-7","0.5.0","2.1.1","1.1.0","1.5.2","1.31.0","1.10.0","0.1.3","0.3.3","3.4.1","0.9.3","0.16.2","1.6.2","1.0.1","4.1.0","4.1.0","4.1.0","2.3","0.3.1","1.2.0","0.10","0.5.4","1.6.1","1.6.9","1.4.4","1.2.3","0.9-13","1.4","1.0.14","0.1.4","1.7.2","1.42","0.4.2","1.3.0","0.20-45","1.7.1","0.2.2","0.11.2","1.0.3","0.9.0","0.9-40","1.2-0","1.9.2","2.0.1","7.3-58.2","1.3-4","4.1.0","1.8-41","0.12","2.1.1","1.2.2.2","1.7","0.5.0","1.1-3","3.1-162","7.3-18","4.1.0","1.34.0","1.8.1","2.0.3","4.10.1","1.8.8","0.8-1","1.18.0","2019.11.13","0.1.1","1.2.0.1","0.4-27","2.2.9","1.0.1","0.4.20","2.5.1","1.0.7","1.0.5","1.4.4","1.0.6","2.20","1.0-11","4.1.19","2.0.3","0.14","0.4.5","1.2.1","1.7.4","4.1.0","4.1.0","4.1.0","1.7.5","1.5.0","3.5-3","3.2.1","1.3.0","1.2.0","0.2.0","4022.108","4.1.0","0.24.3","1.2.2","4.1.0","1.4-11","0.5.2","0.4.1","2.5.0","0.37","1.8-4","0.13.0","2.3.7","1.8-11"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>
</div>

</article> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h1 class="title toc-ignore" style="display:none;">STAT 412-Recitation
8</h1>
<h4 class="author"><em>Ozancan Ozdemir</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</main> <!-- bandContent page -->
</div> <!-- pageContent band -->



<!-- Build Tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
